{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoVG6gfrPpll",
        "outputId": "89c7a999-e09a-4b04-9877-a549fbd94280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/GuviMentor88/Training-Datasets/refs/heads/main/twitter_training.csv', header=None)\n",
        "df.columns = [\"tweet_id\", \"entity\", \"sentiment\", \"tweet\"]\n",
        "df = df[[\"sentiment\", \"tweet\"]]\n",
        "# Download stopwords dataset\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ... (rest of the code)\n",
        "\n",
        "def clean_text(text):\n",
        "    # Check if text is a string before applying lowercase conversion\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # Remove URLs\n",
        "        text = re.sub(r\"@\\w+|\\#\", \"\", text)  # Remove mentions (@username) and hashtags\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "        text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
        "        return text\n",
        "    else:\n",
        "        # Handle non-string values (e.g., return an empty string or NaN)\n",
        "        return \"\"  # or return float('nan')\n",
        "\n",
        "# Apply cleaning to tweets, handling potential errors\n",
        "df[\"clean_tweet\"] = df[\"tweet\"].apply(clean_text)\n",
        "\n",
        "print(df[[\"tweet\", \"clean_tweet\"]].head(10))  # Show cleaned tweets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJFdNF2hQLi9",
        "outputId": "0d202516-f132-40f6-c582-a863e2c3e368"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet  \\\n",
            "0  im getting on borderlands and i will murder yo...   \n",
            "1  I am coming to the borders and I will kill you...   \n",
            "2  im getting on borderlands and i will kill you ...   \n",
            "3  im coming on borderlands and i will murder you...   \n",
            "4  im getting on borderlands 2 and i will murder ...   \n",
            "5  im getting into borderlands and i can murder y...   \n",
            "6  So I spent a few hours making something for fu...   \n",
            "7  So I spent a couple of hours doing something f...   \n",
            "8  So I spent a few hours doing something for fun...   \n",
            "9  So I spent a few hours making something for fu...   \n",
            "\n",
            "                                         clean_tweet  \n",
            "0                      im getting borderlands murder  \n",
            "1                                coming borders kill  \n",
            "2                        im getting borderlands kill  \n",
            "3                       im coming borderlands murder  \n",
            "4                    im getting borderlands 2 murder  \n",
            "5                      im getting borderlands murder  \n",
            "6  spent hours making something fun dont know hug...  \n",
            "7  spent couple hours something fun dont know im ...  \n",
            "8  spent hours something fun dont know im huge bo...  \n",
            "9  spent hours making something fun dont know hug...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert cleaned tweets into numerical features\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Use top 5000 words\n",
        "X = vectorizer.fit_transform(df[\"clean_tweet\"]).toarray()\n",
        "\n",
        "print(\"Shape of TF-IDF transformed data:\", X.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGERnQgkQRai",
        "outputId": "ab7efb71-e9e3-40d6-ca62-22907778cf31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of TF-IDF transformed data: (74682, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"sentiment\"].isna().sum())  # Check number of NaN values\n",
        "print(df[df[\"sentiment\"].isna()])  # Display rows with NaN values\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3mIUtfgDgUX",
        "outputId": "f3f7700d-a466-4e24-de4e-7d26f027240e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Empty DataFrame\n",
            "Columns: [sentiment, tweet, clean_tweet]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdLTs_y99YEG",
        "outputId": "f0056204-7f1d-4ee1-c1bf-f64b092e7d8c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Positive', 'Neutral', 'Negative', 'Irrelevant'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string labels to integers, then remove NaN values\n",
        "#df[\"sentiment\"] = df[\"sentiment\"].map(label_map)\n",
        "\n",
        "# Drop NaN rows after mapping\n",
        "df.dropna(subset=[\"sentiment\"], inplace=True)\n",
        "\n",
        "# Convert to integer type safely\n",
        "#df[\"sentiment\"] = df[\"sentiment\"].astype(int)\n",
        "\n",
        "print(df[\"sentiment\"].isna().sum())  # Should print 0\n",
        "print(df[\"sentiment\"].dtype)  # Should be int\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5HNibYuQyxX",
        "outputId": "7a6bbb8a-499c-4504-fe46-b1653ac27121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string labels to integers\n",
        "label_map = {\"Irrelevant\":3,\"Positive\": 2, \"Neutral\": 1, \"Negative\": 0}\n",
        "df[\"sentiment\"] = df[\"sentiment\"].map(label_map)\n",
        "\n",
        "# Drop rows where sentiment is NaN after mapping\n",
        "df.dropna(subset=[\"sentiment\"], inplace=True)\n",
        "\n",
        "# Convert to integer type\n",
        "df[\"sentiment\"] = df[\"sentiment\"].astype(int)\n"
      ],
      "metadata": {
        "id": "m9q8GRa5RIuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"sentiment\"].isna().sum())  # Should print 0\n",
        "print(df[\"sentiment\"].dtype)  # Should be int\n",
        "print(df[\"sentiment\"].unique())  # Should show [0, 1, 2] only\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E47bzW3jQOmd",
        "outputId": "2aabad87-8c7f-4612-d983-0b4f2cd07baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "int64\n",
            "[2 1 0 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Embedding, Dense, Dropout, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "# Define vocabulary size and sequence length\n",
        "vocab_size = 5000\n",
        "max_length = 100\n",
        "\n",
        "# Tokenize text\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df[\"clean_tweet\"])\n",
        "X_seq = tokenizer.texts_to_sequences(df[\"clean_tweet\"])\n",
        "X_padded = pad_sequences(X_seq, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "# Convert labels to integers\n",
        "#label_map = {\"Positive\": 2, \"Neutral\": 1, \"Negative\": 0}\n",
        "#df[\"sentiment\"] = df[\"sentiment\"].map(label_map).astype(int)\n",
        "# Save tokenizer\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(\"Tokenizer saved as tokenizer.pkl\")\n",
        "# Convert labels to categorical format\n",
        "y = tf.keras.utils.to_categorical(df[\"sentiment\"], num_classes=4)\n",
        "\n",
        "# Split data into training and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define CNN Model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 128, input_length=max_length),\n",
        "    SpatialDropout1D(0.2),\n",
        "    Conv1D(128, 5, activation=\"relu\"),  # 1D Convolution Layer\n",
        "    GlobalMaxPooling1D(),  # Reduces sequence length\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(4, activation=\"softmax\")  # 3-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Save model\n",
        "model.save(\"cnn_sentiment_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnswhwjGRrl9",
        "outputId": "a6975cde-90f8-4a51-874a-55a330d1f362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer saved as tokenizer.pkl\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1868/1868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 50ms/step - accuracy: 0.4928 - loss: 1.1471 - val_accuracy: 0.7088 - val_loss: 0.7496\n",
            "Epoch 2/10\n",
            "\u001b[1m1868/1868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 50ms/step - accuracy: 0.7576 - loss: 0.6484 - val_accuracy: 0.7775 - val_loss: 0.5810\n",
            "Epoch 3/10\n",
            "\u001b[1m1868/1868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 52ms/step - accuracy: 0.8436 - loss: 0.4174 - val_accuracy: 0.8104 - val_loss: 0.5269\n",
            "Epoch 4/10\n",
            "\u001b[1m1868/1868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 50ms/step - accuracy: 0.8804 - loss: 0.3142 - val_accuracy: 0.8168 - val_loss: 0.5253\n",
            "Epoch 5/10\n",
            "\u001b[1m1868/1868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 50ms/step - accuracy: 0.8982 - loss: 0.2627 - val_accuracy: 0.8210 - val_loss: 0.5619\n",
            "Epoch 6/10\n",
            "\u001b[1m1868/1868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 50ms/step - accuracy: 0.9126 - loss: 0.2221 - val_accuracy: 0.8256 - val_loss: 0.5496\n",
            "Epoch 7/10\n",
            "\u001b[1m1868/1868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 50ms/step - accuracy: 0.9165 - loss: 0.2121 - val_accuracy: 0.8236 - val_loss: 0.5965\n",
            "Epoch 8/10\n",
            "\u001b[1m1868/1868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 52ms/step - accuracy: 0.9237 - loss: 0.1938 - val_accuracy: 0.8268 - val_loss: 0.6181\n",
            "Epoch 9/10\n",
            "\u001b[1m1868/1868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 50ms/step - accuracy: 0.9271 - loss: 0.1827 - val_accuracy: 0.8296 - val_loss: 0.6429\n",
            "Epoch 10/10\n",
            "\u001b[1m1868/1868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 52ms/step - accuracy: 0.9264 - loss: 0.1783 - val_accuracy: 0.8326 - val_loss: 0.6508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.models.load_model(\"cnn_sentiment_model.h5\")\n",
        "# Specify either .keras or .h5 extension\n",
        "model.save(\"saved_model.keras\")  # Or model.save(\"saved_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz8W4yTBZ1f0",
        "outputId": "db47cbdc-812b-42bd-bde2-0947052620e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Load test dataset (Replace with actual test data)\n",
        "X_test = [...]  # Your test sequences\n",
        "y_test = [...]  # True labels\n",
        "\n",
        "# Predict sentiment classes\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class=\"ovr\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "lIJFheTydo_-",
        "outputId": "28995fd9-ebcf-449a-e4a7-54508803cdda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized data type: x=[Ellipsis] (of type <class 'list'>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d317e008dab2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predict sentiment classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Compute metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/__init__.py\u001b[0m in \u001b[0;36mget_data_adapter\u001b[0;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unrecognized data type: x={x} (of type {type(x)})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized data type: x=[Ellipsis] (of type <class 'list'>)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oHwLvzOfO6T",
        "outputId": "bc65267d-2638-4965-9c63-773026198a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.43.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.29.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.43.1-py2.py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.43.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mysql.connector\n",
        "\n",
        "# Replace with your actual database details\n",
        "db_config = {\n",
        "    \"host\": \"sentiment-db.cbkegyy2203z.ap-south-1.rds.amazonaws.com\",\n",
        "    \"user\": \"admin\",\n",
        "    \"password\": \"harikrishnanaero\",\n",
        "    \"database\": \"sentiment_analysis\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    conn = mysql.connector.connect(**db_config)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT NOW();\")\n",
        "    result = cursor.fetchone()\n",
        "    print(f\"✅ Successfully connected to MySQL! Server time: {result[0]}\")\n",
        "    conn.close()\n",
        "except Exception as e:\n",
        "    print(f\"❌ Connection failed: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BihmvPqfLwU",
        "outputId": "ac00249b-3c61-440f-c7b4-93971b346268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully connected to MySQL! Server time: 2025-03-09 19:59:16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import mysql.connector\n",
        "from datetime import datetime\n",
        "\n",
        "# Database connection\n",
        "db_config = {\n",
        "    \"host\": \"sentiment-db.cbkegyy2203z.ap-south-1.rds.amazonaws.com\",\n",
        "    \"user\": \"admin\",\n",
        "    \"password\": \"your_password\",\n",
        "    \"database\": \"sentiment_analysis\"\n",
        "}\n",
        "\n",
        "def log_user_login(username):\n",
        "    try:\n",
        "        conn = mysql.connector.connect(**db_config)\n",
        "        cursor = conn.cursor()\n",
        "        query = \"INSERT INTO user_logins (username, login_time) VALUES (%s, %s);\"\n",
        "        cursor.execute(query, (username, datetime.now()))\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        st.success(f\"✅ User {username} login recorded in MySQL!\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"❌ Error logging user login: {e}\")\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Sentiment Analysis Web App\")\n",
        "\n",
        "username = st.text_input(\"Enter Username:\")\n",
        "if st.button(\"Login\"):\n",
        "    log_user_login(username)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RynW71V5fyqs",
        "outputId": "4fbcdca1-5125-41cc-a7e4-e9a26df27226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-09 20:03:24.886 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-09 20:03:24.887 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-09 20:03:24.888 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-09 20:03:24.890 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-09 20:03:24.890 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-09 20:03:24.891 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-09 20:03:24.892 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-09 20:03:24.893 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-09 20:03:24.895 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-09 20:03:24.896 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-09 20:03:24.897 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-09 20:03:24.898 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-03-09 20:03:24.899 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    }
  ]
}